In computer science, Artificial Intelligence (abbreviated AI) is generally the intelligence, reasoning and learning exhibited by machines similar to human reasoning;[1][2] it seeks to develop autonomous machines or expert systems capable of simulating human thought and performing various complex tasks independently.[3][4][5][6][7] It is the system that allows computers to perform advanced functions, such as the ability to analyze large-scale data and make predictions/recommendations;[1][8] It is a field of research in computer science that develops and studies methods and software that allow machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[9] AI began in the 1950s with researchers Alan Turing and Herbert Simon, based on the concept of the Greek philosopher Aristotle.

In 1950, the English mathematician Allan Turing wrote about the possibility of a machine thinking, and imitating intelligent human behavior.[10] He also outlined a research proposal to make this possible.

AI applications include advanced web search engines (e.g. Google Search); recommendation systems (used by YouTube, Amazon and Netflix); virtual assistants (e.g. Google Assistant, Siri and Alexa); autonomous vehicles (e.g. Waymo); generative and creative tools (e.g. ChatGPT, DeepSeek and AI art); and superhuman play and analysis in strategy games (e.g. chess and Go). However, many AI applications are not perceived as AI because they have already become common enough in people's everyday lives.[11][12] For example, optical character recognition (OCR), which extracts text from images; transforms unstructured content into structured data with business-ready insights;[1] translates texts into foreign languages and translates speech.[8] The use of OCR is a key aspect of AI.

AI is a field that encompasses many disciplines, such as computer science, statistics, hardware and software engineering, linguistics, neuroscience and philosophy.[1] Several subfields of AI research are centered on specific goals and the use of specific tools. Traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and robotics support. General intelligence - the ability to complete any task performed by a human at at least an equal level - is among the field's long-term goals.[13] To achieve these goals, AI researchers have adapted and integrated a wide range of techniques, including mathematical and search optimization, formal logic, artificial neural networks and methods based on statistics, operations research and economics. AI also draws on psychology, linguistics, philosophy, neuroscience and other fields[14].

Artificial intelligence was founded as an academic discipline in 1956[15] and the field has gone through multiple cycles of optimism throughout its history,[16] followed by periods of disappointment and loss of funding. Resources and interest increased enormously after 2012, when deep learning surpassed previous AI techniques. This growth accelerated further after 2017[17] and by the early 2020s many billions of dollars were being invested in AI and the field experienced continued rapid progress in what became known as the AI boom. The emergence of advanced generative AI and its ability to create and modify content has exposed various unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, generating discussions about regulatory policies to ensure the safety and benefits of the technology.

Interest in the development of autonomous machines capable of simulating human thought and performing various tasks has grown dramatically over the last few decades,[3][4][5] in the second half of the 20th century, leading to the first studies on artificial intelligence (AI) for a common purpose, based on initiatives by scientists from various fields, such as psychology, cognitive science, computer science and robotics.[18] Efficient tools for analyzing problems and offering solutions and planning (decision making), automating tasks in people's daily lives.[18] The development of artificial intelligence (AI) is a key element in the development of new technologies.

But although the studies are modern, the concept of artificial intelligence is not contemporary; Aristotle (Alexander the Great's teacher) dreamed of replacing slave labor with autonomous tools, and this was possibly the first reported idea of Artificial Intelligence, which computer science would explore much later.[19] The development of this idea took place fully in the 20th century, especially in the 1950s, with thinkers such as Allan Turing, Herbert Simon and John McCarthy. Turing wrote the article "Computing Machinery and Intelligence" about the possibility of a machine thinking and imitating intelligent human behavior with such perfection that it could confuse even a human judge.[20] Turing also outlined a research proposal to make it possible.[20] Initially, the tests in AI were full of successes - but limited due to the reduced performance of the first computers - which caused surprise, was the fact that a computer could perform remotely intelligent activity.[21] The idea of a computer being able to perform intelligent behavior was also discussed.

The initial success continued in 1957 with the General Problem Solver (GPS) developed by Herbert Simon and Allen Newell, a program designed to mimic human problem-solving protocols.[22] Within the limited class of puzzles it could handle, it turned out that the order in which humans approached the same problems was the same.[22] In this way, GPS was perhaps the first program to incorporate the "thinking human" approach.[22] It was also the first program to incorporate the "thinking human" approach.

In 1961, Turing's proposal made a comeback in Herbert Simon and Allen Newell's article "The Simulation of Human Thought" on the testing of a theory of human problem solving.[20] This theory attempts to explain some aspects of the mental processes responsible for human intelligence, a study project known as the Cognitive Simulation Project.[20] It is also known as the Cognitive Simulation Project.

From the outset, the foundations of artificial intelligence were supported by various disciplines that contributed ideas, points of view and techniques to AI. Philosophers (since 400 BC) made AI conceivable by considering the ideas that the mind is, in some respects, similar to a machine, that it operates on knowledge coded in some internal language and that thought can be used to choose the actions that should be performed. In turn, mathematicians provided the tools to manipulate statements of logical certainty, as well as uncertain and probabilistic statements. They also defined the basis for understanding computing and reasoning about algorithms.

Economists have formalized the problem of making decisions that maximize the expected result for the decision-maker. Psychologists have adopted the idea that humans and animals can be considered information processing machines. Linguists have shown that language use fits this model. Computer engineers provide the artifacts that make AI applications possible. AI programs tend to be extensive and could not function without the great advances in speed and memory that the computer industry has provided.

Currently, AI encompasses a huge variety of subfields. Among these subfields is the study of connectionist models or neural networks. A neural network can be seen as a simplified mathematical model of how the human brain works.[23] It consists of a very large number of elementary processing units, or neurons, which receive and send electrical stimuli to each other, forming a highly interconnected network.

In processing, incoming stimuli are composed according to the intensity of each connection, producing a single output stimulus. It is the arrangement of the interconnections between neurons and their respective intensities that defines the main properties and functioning of an ANN. The study of neural networks or connectionism is related to the ability of computers to learn and recognize patterns. We can also highlight the study of molecular biology in an attempt to build artificial life and the area of robotics, linked to biology and seeking to build machines that house artificial life. Another subfield of study is the link between AI and psychology, in an attempt to represent reasoning and search mechanisms in machines.

In recent years, there has been a revolution in work on artificial intelligence, both in terms of content and methodology. It is now more common to use existing theories as a basis rather than proposing entirely new ones, to base information on rigorous theorems or rigid experimental evidence rather than intuition, and to highlight the relevance of real applications rather than hypothetical examples.

The use of AI not only makes it possible to achieve significant performance gains, but also to develop innovative applications capable of expanding our senses and intellectual abilities in extraordinary ways. Increasingly present, artificial intelligence simulates human thought and is spreading into our daily lives. In May 2017, ABRIA (Brazilian Association of Artificial Intelligence) was created in Brazil with the aim of mapping Brazilian initiatives in the artificial intelligence sector, encompassing the efforts of national companies and the training of specialized workers. This step reinforces the fact that artificial intelligence is currently having an impact on the economic sector.

Artificial intelligence began as an experimental field in the 1950s with pioneers such as Allen Newell and Herbert Simon, who founded the first artificial intelligence laboratory at Carnegie Mellon University, and McCarty who, together with Marvin Minsky, founded the MIT AI Lab in 1959. They were some of the participants in the famous 1956 summer conference at Darthmouth College[24].

Historically, there have been two major styles of AI research: "neats" AI and "scruffies" AI.  Neats, clean, classical or symbolic AI. It involves the manipulation of symbols and abstract concepts, and is the methodology used in most forensic systems.

Alongside this approach is the "scruffies" or "connectionist" AI approach, of which neural networks are the best example. This approach creates systems that try to generate intelligence through learning and adaptation rather than creating systems designed with the specific aim of solving a problem. Both approaches appeared at an early stage in the history of AI. In the 1960s and 70s, connectionists were removed from the foreground of AI research, but interest in this aspect of AI was revived in the 1980s, when the limitations of "clean" AI began to be realized.

Research into artificial intelligence was heavily funded in the 1980s by the Defense Advanced Research Projects Agency in the United States and the Fifth Generation Project in Japan. The subsidized work failed to produce immediate results, despite the grandiose promises of some AI practitioners, which led proportionally to major cuts in funding from government agencies in the late 1980s, and consequently to a cooling of activity in the sector, a phase known as The AI Winter. Over the next decade, many AI researchers moved into areas related to more modest goals, such as machine learning, robotics and computer vision, although research into pure AI continued at low levels.

There are two main approaches to the creation of Artificial Intelligence Systems: Symbolism and Connectionism[25].

Symbolism or Symbolic AI,[26] proposes the representation of knowledge through the manipulation of symbols, i.e. in the form of structures built by human beings, usually based on notions of Logic. It had great momentum during a phase when many Expert Systems were created, many of them based on First Order Logic, implemented in Prolog, or in programming languages derived from it or specialized, such as CLIPS. Normally, programs of this type have their knowledge programmed directly by human beings, which led to work on knowledge elicitation. Despite the initial success of Expert Systems, the great difficulty of gathering and recording knowledge from humans and the success of machine learning processes based on data has led to a decline in the importance of this field[26].

Connectionism, or Connectionist AI,[6] is based on a mathematical model inspired by how neurons work,[27] and relies on machine learning based on large masses of data to calibrate this model, which usually starts with random parameters.[28] This approach, despite being proposed very early on, did not find computers capable of modeling complex problems, despite having success with restricted pattern recognition problems, which only happens from the 2010s onwards, with extremely strong results at the end of that decade and at the beginning of the 2020s, from models containing billions of parameters, such as GPT-3[29] and concepts such as Deep Neural Networks,[30] Transformers,[31] and Attention.[32].

By 2022, most AI research will revolve around the concepts of Machine Learning and Connectionism, and there are also proposals for hybrid systems.

The question of what "artificial intelligence" is, even as defined above, can be separated into two parts: "what is the nature of artificial" and "what is intelligence". The first question is relatively easy to resolve, but points to the question of what man can build.

The second question would be considerably more difficult, raising the issue of consciousness, identity and mind (including the unconscious mind) along with the question of what components are involved in the only type of intelligence that is universally accepted as being within the reach of our study: the intelligence of human beings. The study of animals and artificial systems that are not trivial models is beginning to be considered as an agenda for study in the field of intelligence.

When conceptualizing artificial intelligence, interaction with the environment is presumed, in the face of real needs such as relationships between similar individuals, disputes between different individuals, pursuit and flight; as well as specific symbolic communication of cause and effect on various levels of intuitive understanding, whether conscious or not.

Let's suppose a heads or tails competition, the results of which are observed or not. If the second attempt gives the same result as the first, then there were not the same chances for both initial options. Of course, collecting information in just two samples is only reliable because the number of attempts is divisible by the number of likely outcomes.

The truth is that the concept of heads or tails is associated with valuable items such as coins and medals that can prevent people from leaving the game and induce participants to follow the results to the end. In order to maintain the opponent's willingness to challenge the machine, it would be necessary to appear fragile and guarantee the continuation of the game. This is often used in casino machines, where many gamblers can be induced to give up considerable amounts of money.

The use of a results machine can compensate for the absence of an opponent, but in a game of chess, for example, in order for the machine not to need to store all the information that exceeds the capacity of its own imaginable universe, formulas are needed that can be stored so that they can then be calculated by physical, logical, geometric and statistical principles to reflect the complete system in each of its parts; like the integration of Google with Wikipedia, for example.

A popular and early definition of artificial intelligence, introduced by John McCarthy at the famous Dartmouth conference in 1956, is "to make a machine behave in such a way that it would be called intelligent if it were the behavior of a human being." However, this definition seems to ignore the possibility of strong AI (see below).

Another definition of Artificial Intelligence is intelligence that arises from an "artificial device". Most definitions can be categorized into systems that: "think like a human; act like a human; think rationally or act rationally."[6][7]

Leading researchers and textbooks define the field as "the study and design of intelligent agents", where an intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. Andreas Kaplan and Michael Haenlein define artificial intelligence as "a system's ability to correctly interpret external data, learn from that data, and use that learning to achieve specific goals and tasks through flexible adaptation."[33] John McCarthy, who coined the term in 1956 ("at a conference of experts held at Darmouth College" Gubern, Román: The Electronic Eros), defines it as "the science and engineering of producing intelligent systems". It is an area of computing research dedicated to searching for methods or computational devices that possess or multiply the rational capacity of human beings to solve problems, think or, broadly speaking, be intelligent. It can also be defined as the branch of computer science that deals with intelligent behavior[34] or the study of how to make computers do things that humans currently do better[35].

There is no unifying theory or paradigm that guides AI research. Researchers disagree on a number of issues.[36] Some of the longest standing unanswered questions are the following: should artificial intelligence simulate natural intelligence by studying psychology or neuroscience? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering? Can intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems? Can intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require "sub-symbolic" processing?[37] John Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), has also proposed that AI should more appropriately be called synthetic intelligence, a term that has already been adopted by some non-GOFAI researchers.[38] The term "synthetic intelligence" has been coined by some non-GOFAI researchers.

In the 1940s and 1950s, a number of researchers explored the connection between neurology, information theory and cybernetics. Some of them built machines that used electronic networks to display rudimentary intelligence, such as W. Grey Walter's turtles and Johns Hopkins' Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England. In 1960, this approach was abandoned, although its elements were revived in the 1980s.

Interest in neural networks and "connectionism" was revived by David Rumelhart and others in the mid-1980s. These and other sub-symbolic approaches, such as fuzzy systems and evolutionary computation, are now studied collectively by the emerging discipline of computational intelligence.

Weak, limited or narrow AI[39] are intelligent machines or systems that don't do reasoning; it's limited because it's designed to do a specific task, after being trained by a human.[40][41] In other words, it doesn't learn autonomously.[41] For example, recognizing voice commands and finding the fastest route.[39] It's also limited because it's designed to do a specific task, after being trained by a human.

AGI has the ability to understand and adapt to various contexts (and challenges) autonomously without the need for training.[40] Machines with versatile intelligence similar to humans, learn autonomously.[41] AGI has the ability to understand and adapt to various contexts (and challenges) autonomously without the need for training.

Considered the future of AI (speculation), with the creation of machines with the ability to make decisions and analyze data extremely quickly, surpassing strong AI.[40]

They can respond quickly to immediate tasks, but they can't store memory, they haven't improved their functionality with experience (they don't learn).[41] Such as Netflix's recommendation engine, which analyzes the user's history to suggest a film/series.[41]

It is a subfield of artificial intelligence in which a machine learns, i.e. is taught by algorithms to perform a specific task by understanding a pattern[40].

A computational structure inspired by the neural structure of the human being to simulate the human brain, such as the algorithm used in the Google search engine;[40][42] it recognizes the terms searched, searches for synonyms and related subjects in order to generate better results.[42]

It is an intelligent system with the ability to interpret the emotions of people and animals.[41] A term borrowed from psychology, which is the ability of humans to read the emotions of others and predict actions.[41]

Self-awareness, or the singularity point of AI, is a hypothetical stage of artificial intelligence in which machines possess self-awareness. It is a stage beyond the theory of mind and is one of the ultimate goals in the development of AI[41].

It allows machines/systems to converse with humans using human language, such as the use of Alexa and Siri.[40] Personal assistants are part of Artificial Narrow Intelligence (ANI), AI's limitation to perform a set of specific tasks.[42] Navigation systems do well-defined tasks, such as finding the quickest route from one point to another.[42] They are also part of Artificial Narrow Intelligence (ANI).

It analyzes and describes images and videos, can recognize objects and detect movements,[40][41] map a physical environment.[41]

These are intelligent machines or systems used in industry, which have the ability to make decisions/take programmed actions based on the collection of information from the environment (such as humidity and temperature).[40] The ability to identify objects and the ability to navigate different environments autonomously, such as Figure AI, which is building bipedal humanoid robots to work with humans.[41] The ability to identify objects and the ability to navigate different environments autonomously.

AI that creates a realistic human voice, transforming text into voice, modifying timbre, speech intonation and voice speed[40].

They are intelligent systems that can create scripts, poetry, images and videos, not just analyze data[40].

Among the theorists studying what can be done with AI, there is a discussion in which two basic proposals are considered: one known as "strong" and the other known as "weak". Basically, the strong AI hypothesis considers that it is possible to create a conscious machine, i.e. it states that artificial systems should replicate human mentality[43].

Research into Strong Artificial Intelligence addresses the creation of a form of computer-based intelligence that can reason and solve problems; a form of strong AI is classified as self-aware.

Strong AI is a very controversial topic, as it involves issues such as consciousness and strong ethical problems linked to what to do with an entity that is cognitively indistinguishable from human beings.

Science fiction has dealt with many problems of this kind. Isaac Asimov, for example, wrote The Bicentennial Man, where a conscious and intelligent robot struggles to have a status similar to that of a human in society.[44] And Steven Spielberg directed "A.I. Artificial Intelligence" where a robot boy tries to win the love of his "mother", looking for a way to become real. On the other hand, the same Asimov reduces robots to servants of human beings when he proposes the three laws of robotics.[45][46]

Stephen Hawking warned about the dangers of artificial intelligence and considered it a threat to the survival of humanity[47] (see: Rebellion of the machines).

It was the notion of how to deal with non-deterministic problems. One of Alan Turing's practical contributions was what was later called the Turing Test (TT),[48] in 1950: instead of answering the question "can you have intelligent computers?" he formulated his test, which practically became the starting point for research into "Artificial Intelligence"[49].

The test consists of asking questions of a person and a hidden computer. A computer and its programs pass the TT if, from the answers, it is impossible for anyone to distinguish which interlocutor is the machine and which is the person. In his original article, he predicted that by 2000 computers would pass his test.[48] Well, there is an annual competition for TT programs, and the results of the winning systems are so poor (the last one has the name "Ella") that with just a few questions you soon realize the limitations of the machine's answers. It's interesting to note that both the Turing Machine and the Turing Test perhaps derive from Turing's view that the human being is a machine.

There are those who say that this view is absolutely wrong, from a linguistic point of view, since we associate "machine" with an invented and possibly constructed artifact. They say: "No human being was ever invented or built". They also claim that Turing's comparison between man and machine is synonymous with his "social naivety", because machines are infinitely simpler than man, despite the fact that, paradoxically, life is complex. However, this line of reasoning is questionable, after all, modern computers can be considered "complex" when compared to COLOSSUS (a computer whose development was led by Tommy Flowers in 1943), or any machine from the early 20th century.

Weak artificial intelligence focuses its research on creating artificial intelligence that is unable to truly reason and solve problems. Such a machine with this intelligence characteristic would act as if it were intelligent, but it has no self-awareness or self-concept. The classic test for measuring machine intelligence is the Turing Test[48].

There are several fields within weak AI, and one of them is Natural Language Processing, which deals with studying and trying to reproduce the development processes that resulted in the normal functioning of language. Many of these fields use specific software and programming languages created for their purposes. One example is the chatbot Eliza, developed by Joseph Weizenbaum at the MIT Artificial Intelligence Laboratory between 1964 and 1966. Another well-known example is the A.L.I.C.E. program (Artificial Linguistic Internet Computer Entity), software that simulates a human conversation. Programmed in Java and developed with heuristic rules for conversational characters, its development resulted in AIML (Artificial Intelligence Markup Language), a specific language for such programs and their various clones, called Alicebots.

Much of the work in this field has been done with computer simulations of intelligence based on a predefined set of rules. Little progress has been made in strong AI. But depending on the definition of AI used, it can be said that considerable advances in weak AI have already been made.

Many philosophers, above all John Searle and Hubert Dreyfus, have brought philosophical and epistemological issues into the debate, questioning any real possibility of strong AI.[50][51] The very assumptions of building a human-like intelligence or consciousness in a machine would thus be false.[52]

Searle is well known for his counter-argument about the Chinese Room, which inverts the question posed by Minsky about the Turing Test.[53] His argument says that even if a machine can appear to speak Chinese by means of comparative examination with binary reference tables and displays, this does not imply that such a machine actually speaks and understands the language. In other words, demonstrating that a machine can pass the Turing Test does not necessarily imply a conscious being, as understood in its human sense.[54] Dreyfus, in his book What Computers Can't Do Yet: A Critique of Artificial Reasoning, argues that consciousness cannot be acquired by systems based on rules or logic, nor by systems that are not part of a physical body. However, the latter author leaves open the possibility of a robotic system based on Neural Networks, or similar mechanisms, achieving artificial intelligence.[51]

But it would no longer be the aforementioned strong AI, but a much closer correlate of what is understood as weak AI. The setbacks that the primary meaning of Artificial Intelligence has suffered in recent times have contributed to the immediate relativization of its entire legacy. The role of Marvin Minsky, a leading figure at MIT and author of Society of Mind, was central to the idea of a linear AI that would perfectly imitate the human mind, but his main achievement was to build the first computer based on neural networks, known as Snark,[55] which simply failed because it never performed any interesting function, only consumed resources from other more promising research. The first successful neurocomputer (Mark I Perceptron) appeared in 1957 and 1958, created by Frank Rosenblatt, Charles Wightman and others. Today, however, the strands working with the assumptions of emergence and with elements of weak AI seem to have gained prominence in the field.

Criticism of the impossibility of creating intelligence in an artificial compound can be found in Jean-François Lyotard (The Posthuman) and Lucien Sfez (Critique of Communication); a didactic contextualization of the debate can be found in Sherry Turkle (The Second Self: Computers and the Human Spirit). The central argument can be summed up in the fact that the very concept of intelligence is human and, in this sense, animal and biological. The possibility of transporting it to a plastic, artificial basis has a clear and precise limit: if intelligence can be generated from these elements, it must necessarily be different from human intelligence, insofar as its result comes from the emergence of elements that are totally different from those found in humans. Intelligence, as we understand it, is essentially the fruit of the crossing of a biological basis with a symbolic and cultural complex, which is impossible to reproduce artificially.

Other philosophers hold different views. Although they don't see any problems with weak AI, they believe that there are enough elements to believe in strong AI too. Daniel Dennett argues in Consciousness Explained that if there is no magical spark or soul in human beings, then Man is just another machine. Dennett questions why Man-machine should have a privileged position over all other possible machines when provided with intelligence.

Some authors argue that if weak AI is possible, then so is strong AI. The weak AI argument, of an intelligence that is imitated but not real, would thus reveal a supposed validation of strong AI. This would be because, as Simon Blackburn understands in his book Think, among others, there is no possibility of verifying whether an intelligence is real or not. These authors argue that all intelligence only looks like intelligence, without necessarily being so. It is assumed that it is impossible to separate what is actually intelligence from what is merely simulation: it is only believed to be.

These authors rebut the arguments against strong AI by saying that its critics are reduced to arrogant people who cannot understand the origin of life without a magic spark, a God or some superior position. They would ultimately understand the machine as something essentially incapable and cannot even suppose it capable of intelligence. In Minsky's terms, the critique against strong AI errs in assuming that all intelligence derives from a subject - as indicated by Searle - and thus disregards the possibility of complex machinery that could think. But Minsky disregards the simple fact that the greatest advances in the field have been made with "complex machinery", also referred to by leading researchers as Connectionist Artificial Intelligence. If Minsky's criticism were valid, the machine created by Rosenblatt and Bernard Widrow wouldn't still be in use today, and the Mark I Perceptron wouldn't be the founder of neuro-computing. Some leading researchers claim that one of the reasons for Minsky's criticism was the fact that he had failed with Snark. From then on, he began to criticize the field for not fully understanding it, and has since undermined important research on the subject.

The AI debate ultimately reflects the very difficulty contemporary science has in effectively dealing with the absence of a higher primacy. The pro-strong AI arguments shed light on this issue, because it is the scientists themselves, who have tried and failed for decades to create a strong AI, who are still searching for the existence of a higher order. Although strong AI seeks an order within the very conjugation of internal elements, it is still an assumption that there is a superior quality in human intelligence that must be sought, emulated and recreated. It thus reflects the difficult digestion of the radical legacy of the Theory of Evolution, where there is no positivity whatsoever in being human and being intelligent; it is just a complex of relationships that have led to a particular state, the product of a temporal intersection between the biological extract and a symbolic complexity.

It is also argued that artificial intelligence is not yet developed to the point where it can act creatively like the human brain. Furthermore, the human brain is not yet sufficiently understood. Therefore, the idea of replicating human brain functions is currently intangible[56].

AI can be a weapon when used by malicious people, such as the use of the deepfake tool for scams.[57] The creation of these systems can lead to data leaks or accidents with autonomous vehicles.[57] The use of the deepfake tool can also be a weapon.

It has been proven that a qualitative, complete and robust simulator cannot exist, i.e. as long as the input-output vocabulary is used (as in a QSIM algorithm), there will always be input models that cause wrong predictions in their output. For example, the notion of infinity is impossible for a finite machine (computer or neurons if they only produce a finite number of results in a finite amount of time). In this case, it's a simple mathematical paradox, because there are a finite number of combinations coming out of any finite set. If the notion of infinity could be obtained by a certain finite combination, this would mean that infinity would be equivalent to this finite sequence, which is obviously a contradiction. Therefore, infinity and other abstract notions have to be pre-acquired in a finite machine, they are not programmable there."[58][59].

Artificial intelligence, in a broad context, has diverse applications and is used to solve practical problems by civil, government and military entities. It has applications in healthcare, media and e-commerce, among others.[60] There is discussion of how AI has been integrated into automated planning systems, medical diagnosis, use by lawyers (AI for lawyers), language recognition and much more, showing the wide range of applications and the profound impact of AI in multiple areas.

There are currently a number of AI researchers around the world in various institutions and research companies. Among the many who have made significant contributions are:

British mathematician Alan Turing was one of the most important men not only for his time, but for today. His studies were not only the basis for the existence of artificial intelligence, but also for almost every electronic device ever made. He created the "Turing Test", which is still used today to find out the level of intelligence of an artificial intelligence program. This test was not created to analyze a computer's ability to think for itself, since machines are completely incapable of that, but rather to identify how well it can imitate the human brain.

Mathematician, scientist, creator of the term "artificial intelligence" and also the father of the LISP programming language. McCarthy was considered one of the first men to work on the development of artificial intelligence and always said that it should interact with man. Born in Boston, he worked at Stanford University and the Massachusetts Institute of Technology (MIT) and won the Turing Award in 1972 and the National Medal of Science in 1991. LISP programming, one of McCarthy's greatest achievements, appeared in 1958 and served to facilitate the development of artificial intelligence. The language is one of the oldest still in use and was first used when a computer was set up to play chess against a human opponent.

Born in New York, the scientist received several international awards for his pioneering work in the field of artificial intelligence, including the Turing Award in 1969, the highest award in computer science. The scientist explored how to endow machines with human-like perception and intelligence, created robotic hands capable of manipulating objects, developed new programming frameworks and wrote about philosophical issues related to artificial intelligence. Minsky was convinced that man would one day develop machines that would compete with his intelligence and saw the brain as a machine whose functioning can be studied and reproduced on a computer, which could help to better understand the human brain and higher mental functions.

An Indian computer scientist naturalized in the United States, he was the first Asian to win the Turing Award. Among his contributions to AI are the creation of CMU's Robotics Institute and demonstrations of various systems that use some form of AI. These include: speech systems, voice-controlled systems, voice recognition, speaker-independent voice recognition, etc. For Reddy, rather than replacing humanity, technology will create a new type of human that will coexist with its predecessors while taking advantage of a new class of tools made possible by technology.

Winograd is an American computer scientist, professor at Stanford University, and co-director of Stanford's human-computer interaction group. He is known in the fields of philosophy of mind and artificial intelligence for his work on natural language using the SHRDLU program. For Terry, there is no doubt that computer technology, more precisely the field of artificial intelligence, will transform societies, introducing irreversible socio-economic changes. This expert wants to know whether human beings would be able to build machines that could understand them, solve their problems and run their lives, as well as looking for answers as to what would happen if, one day, these machines became more intelligent than the humans who created them.

Born in Philadelphia, Pennsylvania, he graduated from the University of Pennsylvania. Douglas Bruce Lenat is the CEO of Cycorp and has also been a prominent researcher in artificial intelligence, receiving the biannual IJCAI Computers and Thought award in 1976 for the creation of the machine learning program. He also worked on military simulations and numerous projects for US government, military, scientific and intelligence organizations. Lenat's mission in the long cycle of the Cyc project, which began in 1984, was to build the basis of a general artificial intelligence by manually representing knowledge as contextualized logical axioms in formal language based on extensions to the first-order predicate calculus, and then using this huge ontology inference engine and contextualized knowledge base as an inductive bias to automate and increasingly accelerate Cyc's own continuing education via machine learning and natural language understanding.